#!/usr/bin/env python3

from multiprocessing import cpu_count

import click
@click.command()
@click.option('-v', '--verbosity', count=True, help="Verbosity level")
@click.option('--mjd', default=None, type=int, help="Modified Julian date to query. Use negative values to indicate relative to current MJD.")
@click.option('--mjd-start', default=None, type=int, help="Start of MJD range to query")
@click.option('--mjd-end', default=None, type=int, help="End of MJD range to query")
@click.option('--date', default=None, type=str, help="Date to query (e.g., 2024-01-15)")
@click.option('--date-start', default=None, type=str, help="Start of date range to query")
@click.option('--date-end', default=None, type=str, help="End of date range to query")
@click.option('--all', "all_time", is_flag=True, help="Include all MJDs")
@click.option('--apo', is_flag=True, help="Query Apache Point Observatory data")
@click.option('--lco', is_flag=True, help="Query Las Campanas Observatory data")
@click.option('--processes', '-p', default=cpu_count(), type=int, help="Number of processes to use")
def sync(verbosity, mjd, mjd_start, mjd_end, date, date_start, date_end, all_time, apo, lco, processes):
    """
    Scrape metadata from raw exposures and update the almanac database.
    """
    
    from peewee import chunked, fn
    from tqdm import tqdm
    from itertools import product

    from almanac import (apogee, io, utils)
    from almanac.models.apogee import Exposure, Sequence

    from almanac.models.base import database

    database.create_tables([Exposure, Sequence])

    tqdm_kwds = dict(unit="night", disable=(verbosity < 1))
    
    mjds = utils.parse_mjds(mjd, mjd_start, mjd_end, date, date_start, date_end, all_time, earliest_mjd=55139) # APOGEE specific
    observatories = utils.get_observatories(apo, lco)
    iterable = product(observatories, mjds)    

    # TODO: Skip over observatory/mjds for which we already have something in the Exposure table?
    exposure_kwds = []
    if processes is not None and processes > 1:         
        # Parallel
        import os
        import signal
        import concurrent.futures
        with concurrent.futures.ProcessPoolExecutor(max_workers=processes) as pool:
            futures = []
            for total, (observatory, mjd) in enumerate(iterable, start=1):
                futures.append(pool.submit(apogee.get_exposure_metadata_as_list, observatory, mjd))

            with tqdm(total=total, **tqdm_kwds) as pb:
                try:                
                    for future in concurrent.futures.as_completed(futures):
                        pb.update()
                        r = future.result()
                        if r is None or len(r) == 0: 
                            continue                
                        exposure_kwds.extend(list(map(apogee.parse_exposure_metadata, r)))

                except KeyboardInterrupt:
                    for pid in pool._processes:
                        os.kill(pid, signal.SIGKILL)
                    pool.shutdown(wait=False, cancel_futures=True)                
                    raise KeyboardInterrupt
    
    else:
        for observatory, mjd in tqdm(iterable, total=len(mjds) * len(observatories), **tqdm_kwds):
            for result in apogee.get_exposure_metadata(observatory, mjd):
                exposure_kwds.append(apogee.parse_exposure_metadata(result))

    # Bulk create the exposures.
    # On conlict, update everything except the unique keys

    conflict_target = (
        Exposure.observatory,
        Exposure.mjd,
        Exposure.exposure,
        Exposure.prefix
    )
    ignore_field_names = [t.name for t in conflict_target]
    preserve_fields = list(
        field
        for field in Exposure._meta.fields.values()
        if field.name not in ignore_field_names
    )

    batch_size = 1_000
    for chunk in chunked(exposure_kwds, batch_size):
        try:
            (
                Exposure
                .insert_many(chunk)
                .on_conflict(
                    conflict_target=conflict_target,
                    preserve=preserve_fields
                )
                .execute()
            )
        except:
            # Save each exposure individually to forcibly re-raise the relevant exception
            for kwds in chunk:
                (
                    Exposure
                    .insert(**kwds)
                    .on_conflict()
                    .execute()
                )

    # Create sequences based on all exposures
    # TODO: restrict this to MJDs/observatory where we have no sequences?
    sq = (
        Exposure
        .select(
            (
                (
                    fn.row_number()
                    .over(
                        partition_by=[Exposure.mjd, Exposure.observatory],
                        order_by=[Exposure.exposure]
                    )
                )
                - 
                (
                    fn.row_number()
                    .over(
                        partition_by=[Exposure.mjd, Exposure.observatory, Exposure.exp_type],
                        order_by=[Exposure.exposure]
                    )
                )        
            ).alias("group_index"),
            Exposure.pk,
            Exposure.mjd,
            Exposure.observatory,
            Exposure.exposure,
            Exposure.exp_type,
        )
        .where(Exposure.exp_type.is_null(False))
        .order_by(
            Exposure.mjd.asc(),
            Exposure.observatory.asc(),
            Exposure.exposure.asc()
        )
        .alias("sq")
    )

    q = (
        Exposure
        .select(
            Exposure.mjd,
            Exposure.exp_type,
            Exposure.observatory,
            fn.min(Exposure.exposure).alias("first_exposure"),
            fn.max(Exposure.exposure).alias("last_exposure"),
            fn.count(Exposure.pk).alias("n_exposures")
        )
        .join(sq, on=(sq.c.pk == Exposure.pk))
        .group_by(Exposure.mjd, Exposure.exp_type, sq.c.group_index)
        .order_by(Exposure.mjd, fn.min(Exposure.exposure))
        .dicts()
    )

    sequence_fields = [Sequence.mjd, Sequence.exp_type, Sequence.first_exposure, Sequence.last_exposure]
    for chunk in chunked(q, batch_size):
        (
            Sequence
            .insert_many(chunk)
            .on_conflict(
                conflict_target=sequence_fields,
                preserve=sequence_fields
            )
            .execute()
        )
        
    # Now get fiber information where we need it.

if __name__ == '__main__':
    sync()
